{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!curl --user \"USERNAME:PASSWORD\" https://static.miglix.eu/data/Task01_BrainTumour.tar | tar xf -"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!curl -O --user \"USERNAME:PASSWORD\" https://static.miglix.eu/data/checkpoint_sd.pt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!rm -rf brats-net\n","!git clone https://github.com/hesiod/brats-net.git\n","!rm -f model\n","!ln -s brats-net/model ."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.utils.data as td\n","import torch.nn as nn\n","from torch.utils.tensorboard import SummaryWriter\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import model.brats_dataset\n","import model.unet\n","import model.utils\n","import model.loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Context:\n","    def __init__(self, filename=None, criterion=None):\n","        self.net = model.unet.Net()\n","        if filename:\n","            self.net.load_state_dict(torch.load(filename))\n","        else:\n","            self.net.apply(model.unet.init_weights)\n","\n","        self.device = model.utils.try_gpu()\n","        print('Using device {}'.format(self.device))\n","\n","        self.run_iter = 0\n","\n","    def check_topology(self):\n","        dummy_input = torch.randn(size=(2, 4, 240, 240), dtype=torch.float32)\n","        y = self.net(dummy_input)\n","\n","\n","    def export_onnx(self, filename):\n","        dummy_input = torch.randn(1, 4, 240, 240)\n","        torch.onnx.export(self.net, dummy_input, filename, verbose=True)\n","\n","class TrainContext:\n","    def __init__(self, context, data_context, criterion, lr, batch_size, experiment_name='unnamed'):\n","        self.ctx = context\n","        self.data = data_context\n","        self.global_iter = 1\n","        self.experiment_name = experiment_name\n","\n","        self.criterion = criterion\n","        self.batch_size = batch_size\n","\n","        self.optimizer = torch.optim.SGD(self.ctx.net.parameters(), lr=lr)\n","        #optimizer = torch.optim.RMSprop(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)\n","        #self.scheduler = ReduceLROnPlateau(self.optimizer, 'min')\n","\n","    def run(self, num_epochs):\n","        self.ctx.run_iter += 1\n","        run_name = 'runs/run_{}_{}_{}'.format(self.experiment_name, lr, self.ctx.run_iter)\n","\n","        print('Commencing {}'.format(run_name))\n","        self.writer = SummaryWriter(os.path.join('runs', run_name))\n","\n","        print('Writing graph')\n","        dummy_input = torch.randn(size=(2, 4, 240, 240), dtype=torch.float32)\n","        self.writer.add_graph(self.ctx.net, dummy_input)\n","        del dummy_input\n","        print('done')\n","\n","        self.ctx.net.to(self.ctx.device)\n","\n","        self.brats_train_perepoch = self.data.split_data(num_epochs)\n","\n","        for epoch in range(num_epochs-1):\n","            self.run_epoch(epoch)\n","\n","        self.writer.close()\n","\n","    def checkpoint(self):\n","        torch.save(self.ctx.net.state_dict(), 'checkpoints/checkpoint_{}_{}.pt'.format(self.experiment_name, self.ctx.run_iter))\n","\n","    def run_epoch(self, epoch):\n","        train_iter = td.DataLoader(self.brats_train_perepoch[epoch], batch_size, shuffle=True, num_workers=num_workers)\n","        test_iter = td.DataLoader(self.brats_train_perepoch[epoch+1], batch_size, shuffle=True, num_workers=num_workers)\n","        #test_iter = td.DataLoader(brats_test, batch_size, shuffle=False, num_workers=num_workers)\n","        \n","        batch_count = len(train_iter)\n","        \n","        print('Total batches: {}'.format(batch_count))\n","\n","        train_loss_epoch = 0.0\n","        for i, (X, y) in enumerate(train_iter):\n","            batch_loss = self.run_batch(i, X, y)\n","            train_loss_epoch += batch_loss\n","            print('batch {:4}/{} batchloss {}'.format(i+1, batch_count, batch_loss))\n","        train_loss_epoch /= batch_count\n","\n","        self.writer.add_scalar('loss/train', train_loss_epoch, self.global_iter)\n","\n","        self.ctx.net.eval()\n","        test_loss_epoch = 0.0\n","        with torch.no_grad():\n","            for X_test, y_test in test_iter:\n","                X_test = X_test.float().to(self.ctx.device)\n","                y_test = y_test.float().to(self.ctx.device)\n","                y_test_hat = self.ctx.net(X_test).squeeze(1)\n","                b_l = self.criterion(y_test_hat, y_test)\n","                test_loss_epoch += float(b_l)\n","            test_loss_epoch /= len(test_iter)\n","\n","        self.writer.add_scalar('loss/test', test_loss_epoch, self.global_iter)\n","        self.writer.flush()\n","\n","        self.checkpoint()\n","\n","\n","        print('epoch {}/{}, train loss {}, test loss {}'.format(epoch+1,\n","            num_epochs, self.train_loss_epoch, test_loss_epoch))\n","\n","    def run_batch(self, i, X, y):\n","        self.global_iter += 1\n","        self.ctx.net.train()\n","\n","        X = X.float().to(self.ctx.device)\n","        y = y.float().to(self.ctx.device)\n","        y_hat = self.ctx.net(X).squeeze(1)\n","\n","        l = self.criterion(y_hat, y)\n","\n","        self.optimizer.zero_grad()\n","        l.backward()\n","        nn.utils.clip_grad_value_(self.ctx.net.parameters(), 0.1)\n","        self.optimizer.step()\n","\n","        for tag, value in self.ctx.net.named_parameters():\n","            tag = tag.replace('.', '/')\n","            self.writer.add_histogram('weights/' + tag, value.data.cpu().numpy(), self.global_iter)\n","            self.writer.add_histogram('grads/' + tag, value.grad.data.cpu().numpy(), self.global_iter)\n","\n","        self.writer.add_scalar('loss/total_loss', float(l), self.global_iter)\n","        self.writer.add_images('masks/0_base', X[:, 0:3, :, :], self.global_iter)\n","        y_us = y.unsqueeze(1)\n","        y_hat_us = y_hat.unsqueeze(1)\n","        y_hat_us_sig = torch.sigmoid(y_hat_us) > 0.5\n","        self.writer.add_images('masks/1_true', y_us, self.global_iter)\n","        self.writer.add_images('masks/2_predicted', y_hat_us_sig, self.global_iter)\n","        self.writer.add_images('extra/raw', y_hat_us, self.global_iter)\n","        overlaid = torch.cat([y_hat_us_sig.float(), y_us, torch.zeros_like(y_us)], dim=1)\n","        self.writer.add_images('extra/overlaid', overlaid, self.global_iter)\n","\n","        self.writer.flush()\n","\n","        return float(l)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dctx = model.brats_dataset.DataSplitter()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Identifier for this group of runs\n","meta_name = 'brats4'\n","batch_size = 32\n","num_workers = 6\n","num_epochs = 50\n","lr = 0.0001\n","should_check = False"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ctx = Context('checkpoint_sd.pt')\n","\n","if should_check:\n","    # Check whether layer inputs/outputs dimensions are correct\n","    # by conducting a test run\n","    ctx.check_topology()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["criterion = model.loss.Loss()\n","\n","tctx = TrainContext(ctx, dctx, criterion=criterion, lr=lr, batch_size=batch_size, experiment_name=meta_name)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tctx.run(num_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ctx.export_onnx(\"net5.onnx\")"]}],"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2},"nbformat":4,"nbformat_minor":2}